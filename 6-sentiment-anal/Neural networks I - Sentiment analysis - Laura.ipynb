{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc9be56",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b3b52",
   "metadata": {},
   "source": [
    "In this exercise we use the IMDb-dataset, which we will use to perform a sentiment analysis. The code below assumes that the data is placed in the same folder as this notebook. We see that the reviews are loaded as a pandas dataframe, and print the beginning of the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67da3bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                   0\n",
      "0  bromwell high is a cartoon comedy . it ran at ...\n",
      "1  story of a man who has unnatural feelings for ...\n",
      "2  homelessness  or houselessness as george carli...\n",
      "3  airport    starts as a brand new luxury    pla...\n",
      "4  brilliant over  acting by lesley ann warren . ...\n",
      "          0\n",
      "0  positive\n",
      "1  negative\n",
      "2  positive\n",
      "3  negative\n",
      "4  positive\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('reviews.txt', header=None)\n",
    "labels = pd.read_csv('labels.txt', header=None)\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "\n",
    "print(type(reviews))\n",
    "print(reviews.head())\n",
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982b946",
   "metadata": {},
   "source": [
    "**(a)** Split the reviews and labels in test, train and validation sets. The train and validation sets will be used to train your model and tune hyperparameters, the test set will be saved for testing. Use the `CountVectorizer` from `sklearn.feature_extraction.text` to create a Bag-of-Words representation of the reviews. Only use the 10,000 most frequent words (use the `max_features`-parameter of `CountVectorizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f58edbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "\n",
    "revs_array = reviews[0].values\n",
    "\n",
    "# Fit and transform the text data\n",
    "bag = vectorizer.fit_transform(revs_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf07ee9",
   "metadata": {},
   "source": [
    "**(b)** Explore the representation of the reviews. How is a single word represented? How about a whole review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84f1809f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3156666 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag\n",
    "\n",
    "# Inspecting the output below, which is the data-type of the representation of the reviews,\n",
    "# we see that it is a sparse matrix. Let's take a closer look below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2abf01b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       aaron  abandon  abandoned  abc  abilities  ability  able  aboard  \\\n",
      "24995      0        0          0    0          0        0     0       0   \n",
      "24996      0        0          0    0          0        0     1       0   \n",
      "24997      0        0          0    0          0        0     0       0   \n",
      "24998      0        0          0    0          0        0     0       0   \n",
      "24999      0        0          0    0          0        0     0       0   \n",
      "\n",
      "       abominable  abomination  ...  zhang  zizek  zodiac  zombi  zombie  \\\n",
      "24995           0            0  ...      0      0       0      0       0   \n",
      "24996           0            0  ...      0      0       0      0       0   \n",
      "24997           0            0  ...      0      0       0      0       2   \n",
      "24998           0            0  ...      0      0       0      0       0   \n",
      "24999           0            0  ...      0      0       0      0       0   \n",
      "\n",
      "       zombies  zone  zoom  zorro  zu  \n",
      "24995        0     0     0      0   0  \n",
      "24996        0     0     0      0   0  \n",
      "24997        0     0     0      0   0  \n",
      "24998        0     0     0      0   0  \n",
      "24999        0     0     0      0   0  \n",
      "\n",
      "[5 rows x 10000 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# convert to a DataFrame for visualization\n",
    "df = pd.DataFrame(bag.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd5ba7",
   "metadata": {},
   "source": [
    "We see that the bag-of-words representation of a review has each word of the vocabulary (at least the 10000 most frequent ones) as a row and then their occurrence in each review (which are the columns) as an integer. So the value we will see under the column of each word will be the amount of times it occurrs in each review. This is why the matrix is so scarce - there are 10000 words in our \"vocabulary\" and each review probably only uses a very small percentage of that.\n",
    "\n",
    "We see in the tail of the reviews that review number 24997 said the word \"zombie\" twice, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15b0cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X = bag\n",
    "Y = to_categorical(Y, 2)\n",
    "# splitting data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2638fce",
   "metadata": {},
   "source": [
    "**(c)** Train a neural network with a single hidden layer on the dataset, tuning the relevant hyperparameters to optimize accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0cc1d53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6118 - loss: 0.6500 - val_accuracy: 0.7423 - val_loss: 0.5583\n",
      "Epoch 2/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7490 - loss: 0.5434 - val_accuracy: 0.7890 - val_loss: 0.4900\n",
      "Epoch 3/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7926 - loss: 0.4818 - val_accuracy: 0.8097 - val_loss: 0.4445\n",
      "Epoch 4/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8179 - loss: 0.4384 - val_accuracy: 0.8283 - val_loss: 0.4142\n",
      "Epoch 5/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8334 - loss: 0.4063 - val_accuracy: 0.8337 - val_loss: 0.3926\n",
      "Epoch 6/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8452 - loss: 0.3810 - val_accuracy: 0.8440 - val_loss: 0.3761\n",
      "Epoch 7/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8527 - loss: 0.3604 - val_accuracy: 0.8483 - val_loss: 0.3629\n",
      "Epoch 8/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8619 - loss: 0.3430 - val_accuracy: 0.8483 - val_loss: 0.3521\n",
      "Epoch 9/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8690 - loss: 0.3278 - val_accuracy: 0.8527 - val_loss: 0.3431\n",
      "Epoch 10/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8747 - loss: 0.3145 - val_accuracy: 0.8553 - val_loss: 0.3357\n",
      "Epoch 11/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8823 - loss: 0.3026 - val_accuracy: 0.8590 - val_loss: 0.3295\n",
      "Epoch 12/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8879 - loss: 0.2918 - val_accuracy: 0.8623 - val_loss: 0.3244\n",
      "Epoch 13/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8923 - loss: 0.2820 - val_accuracy: 0.8643 - val_loss: 0.3202\n",
      "Epoch 14/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8976 - loss: 0.2727 - val_accuracy: 0.8670 - val_loss: 0.3170\n",
      "Epoch 15/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9013 - loss: 0.2638 - val_accuracy: 0.8680 - val_loss: 0.3149\n",
      "Epoch 16/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9051 - loss: 0.2551 - val_accuracy: 0.8697 - val_loss: 0.3138\n",
      "Epoch 17/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9084 - loss: 0.2465 - val_accuracy: 0.8707 - val_loss: 0.3139\n",
      "Epoch 18/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9114 - loss: 0.2382 - val_accuracy: 0.8713 - val_loss: 0.3156\n",
      "Epoch 19/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9151 - loss: 0.2303 - val_accuracy: 0.8713 - val_loss: 0.3190\n",
      "Epoch 20/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9175 - loss: 0.2230 - val_accuracy: 0.8700 - val_loss: 0.3242\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "from numpy.random import seed, randint\n",
    "\n",
    "seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "model = Sequential() #initialize neural network\n",
    "model.add(Dense(units = 30, activation = 'sigmoid', input_dim = 10000)) #add the first hidden layer\n",
    "model.add(Dense(units = 2, activation = 'softmax')) #output layer\n",
    "\n",
    "sgd = optimizers.SGD(learning_rate = 0.03)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 20, batch_size = 50, validation_split = 0.2, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4106b76d",
   "metadata": {},
   "source": [
    "I messed around with the Learning Rate, Curve type, number of epochs and batch size until I achieved 91% accuracy on the test set and 87% accuracy on the validation set. I was keeping my eye mostly on the validation set and this was the absolute maximum I managed to get across all experiments.\n",
    "\n",
    "What happened first was that I was looking at the train accuracy and realized that if I added enough epochs it would eventually reach 100% and not go anywhere. However, I then started keeping my eye in the validation accuracy and realized that this one \"plateau'd\" far sooner, which made me reduce the epoch number even further.\n",
    "\n",
    "I then realized that if my learning rate was too high, there would be considerate fluctuations (both up and down) across my epochs. Considering that I had so little, I decided to reduce the learning rate so that I would have a higher chance of increasing the accuracy with each epoch. This worked out well for the hyperparameters I ended up going with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd327a6",
   "metadata": {},
   "source": [
    "**(d)** Test your sentiment-classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7a70c996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - accuracy: 0.8599 - loss: 0.3350\n",
      "Test Loss: 0.3363731801509857\n",
      "Test Accuracy: 0.8611999750137329\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154becb6",
   "metadata": {},
   "source": [
    "It seems that the test accuracy is extremely similar to the validation accuracy, which is good!\n",
    "\n",
    "It means that the model is good at generalizing on unseen data and that the split between train/test/validation was good as well, since there was no considerable fluctutation between the test and validation accuracy results.\n",
    "\n",
    "Since I couldn't bring my validation accuracy any higher, I also didn't expect the test accuracy to go higher than what we got in the previous step.\n",
    "\n",
    "I am happy with these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44ee62",
   "metadata": {},
   "source": [
    "**(e)** Use the classifier to classify a few sentences you write yourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a1ef2970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "[0.86603284 0.13396718]\n",
      "Original Text: This movie was terrible! The acting was awful.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.28333795 0.71666205]\n",
      "Original Text: Amazing movie!\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.43251127 0.56748873]\n",
      "Original Text: Cate Blanchett\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.29950377 0.7004962 ]\n",
      "Original Text: I have seen more boobs in this movie than in my life\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.3316189 0.6683811]\n",
      "Original Text: I love the authenticity of the actors\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.3597052  0.64029485]\n",
      "Original Text: a lot of violence\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.5069899  0.49301013]\n",
      "Original Text: too much violence\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.46559173 0.5344083 ]\n",
      "Original Text: i hated this movie\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.64204276 0.35795733]\n",
      "Original Text: Adam Sandler is ugly and acts bad\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.39344728 0.6065528 ]\n",
      "Original Text: I actually cried\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.2951782 0.7048217]\n",
      "Original Text: beautiful animation\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.44732192 0.5526781 ]\n",
      "Original Text: This studio only does shitty crappy movies\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.618675   0.38132504]\n",
      "Original Text: terrible\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.60251623 0.39748377]\n",
      "Original Text: This studio only does terrible movies\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.41482466 0.5851752 ]\n",
      "Original Text: I hate this movie\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.46559173 0.5344083 ]\n",
      "Original Text: I hated this movie\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.5584128 0.4415872]\n",
      "Original Text: I hated hated hated hated this movie\n",
      "Predicted Sentiment: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the new text\n",
    "new_text = [\n",
    "    \"This movie was terrible! The acting was awful.\",\n",
    "    \"Amazing movie!\",\n",
    "    \"Cate Blanchett\",\n",
    "    \"I have seen more boobs in this movie than in my life\",\n",
    "    \"I love the authenticity of the actors\",\n",
    "    \"a lot of violence\",\n",
    "    \"too much violence\",\n",
    "    \"i hated this movie\",\n",
    "    \"Adam Sandler is ugly and acts bad\",\n",
    "    \"I actually cried\",\n",
    "    \"beautiful animation\",\n",
    "    \"This studio only does shitty crappy movies\",\n",
    "    \"terrible\",\n",
    "    \"This studio only does terrible movies\",\n",
    "    \"I hate this movie\",\n",
    "    \"I hated this movie\",\n",
    "    \"I hated hated hated hated this movie\"]\n",
    "\n",
    "# Vectorize the text using the same vectorizer used during training\n",
    "new_text_vectorized = vectorizer.transform(new_text)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(new_text_vectorized)\n",
    "\n",
    "# Loop through each prediction\n",
    "for i, pred in enumerate(predictions):\n",
    "    # Determine sentiment label\n",
    "    print(pred)\n",
    "    sentiment = \"positive\" if pred[0] < 0.5 else \"negative\"\n",
    "    # Print the original text and the predicted sentiment\n",
    "    print(f\"Original Text: {new_text[i]}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c22d0a",
   "metadata": {},
   "source": [
    "By printing out the predicted sentiment alongside the model's confidence for each of these reviews, we can conclude that the \"Bag of Words\" method might not be the best way to go for this exercise because perhaps it removes emphasis from the words that actually matter for the sentiment of the review. An average person would immediatelly determine \"I hated this movie\" and \"This studio only does shitty crappy movies\" as negative reviews, but the model, despite being a bit \"on edge\" (only around 60% sure), still classified them as positive.\n",
    "\n",
    "From testing with my own sentences, I also realized that the amount of times that a word appears in a review greatly influences the output of this model (\"I hated this movie\" being classified as positive but \"I hated hated hated hated this movie\" being classified as negative).\n",
    "\n",
    "One way to make this model more robust would be to remove neutral words from the bag of words (for example \"movie\", \"this\", \"is\") and all of these words that are required to make a sentence make sense. Then, the remaining words would carry far more meaning to the model, perhaps making it perform a lot better (Because the sentence \"I hated this movie\" would have just become \"hated\" and be far more likely to be classified as negative).\n",
    "\n",
    "I would also like to note that perhaps the reason why the model didn't perform so well on my hand-written reviews is because they are drastically shorter than those we see on imdb, so this type of data might be a bit foreign to our model ahahahah.\n",
    "\n",
    "This exercise/assignment was very interesting! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
