{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc9be56",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b3b52",
   "metadata": {},
   "source": [
    "In this exercise we use the IMDb-dataset, which we will use to perform a sentiment analysis. The code below assumes that the data is placed in the same folder as this notebook. We see that the reviews are loaded as a pandas dataframe, and print the beginning of the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67da3bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                   0\n",
      "0  bromwell high is a cartoon comedy . it ran at ...\n",
      "1  story of a man who has unnatural feelings for ...\n",
      "2  homelessness  or houselessness as george carli...\n",
      "3  airport    starts as a brand new luxury    pla...\n",
      "4  brilliant over  acting by lesley ann warren . ...\n",
      "          0\n",
      "0  positive\n",
      "1  negative\n",
      "2  positive\n",
      "3  negative\n",
      "4  positive\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('reviews.txt', header=None)\n",
    "labels = pd.read_csv('labels.txt', header=None)\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "\n",
    "print(type(reviews))\n",
    "print(reviews.head())\n",
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982b946",
   "metadata": {},
   "source": [
    "**(a)** Split the reviews and labels in test, train and validation sets. The train and validation sets will be used to train your model and tune hyperparameters, the test set will be saved for testing. Use the `CountVectorizer` from `sklearn.feature_extraction.text` to create a Bag-of-Words representation of the reviews. Only use the 10,000 most frequent words (use the `max_features`-parameter of `CountVectorizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f58edbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "revs_array = reviews[0].values\n",
    "bag = vectorizer.fit_transform(revs_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf07ee9",
   "metadata": {},
   "source": [
    "**(b)** Explore the representation of the reviews. How is a single word represented? How about a whole review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84f1809f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3156666 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag\n",
    "# Inspecting the output below, which is the data-type of the representation of the reviews,\n",
    "# we see that it is a sparse matrix. Let's take a closer look below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2abf01b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       aaron  abandon  abandoned  abc  abilities  ability  able  aboard  \\\n",
      "24995      0        0          0    0          0        0     0       0   \n",
      "24996      0        0          0    0          0        0     1       0   \n",
      "24997      0        0          0    0          0        0     0       0   \n",
      "24998      0        0          0    0          0        0     0       0   \n",
      "24999      0        0          0    0          0        0     0       0   \n",
      "\n",
      "       abominable  abomination  ...  zhang  zizek  zodiac  zombi  zombie  \\\n",
      "24995           0            0  ...      0      0       0      0       0   \n",
      "24996           0            0  ...      0      0       0      0       0   \n",
      "24997           0            0  ...      0      0       0      0       2   \n",
      "24998           0            0  ...      0      0       0      0       0   \n",
      "24999           0            0  ...      0      0       0      0       0   \n",
      "\n",
      "       zombies  zone  zoom  zorro  zu  \n",
      "24995        0     0     0      0   0  \n",
      "24996        0     0     0      0   0  \n",
      "24997        0     0     0      0   0  \n",
      "24998        0     0     0      0   0  \n",
      "24999        0     0     0      0   0  \n",
      "\n",
      "[5 rows x 10000 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# convert to a DataFrame for visualization\n",
    "df = pd.DataFrame(bag.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd5ba7",
   "metadata": {},
   "source": [
    "We see that the bag-of-words representation of a review has each word of the vocabulary (at least the 10000 most frequent ones) as a row and then their occurrence in each review (which are the columns) as an integer. So the value we will see under the column of each word will be the amount of times it occurrs in each review. This is why the matrix is so scarce - there are 10000 words in our \"vocabulary\" and each review probably only uses a very small percentage of that.\n",
    "\n",
    "We see in the tail of the reviews above that review number 24997 said the word \"zombie\" twice, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15b0cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X = bag\n",
    "Y = to_categorical(Y, 2)\n",
    "# splitting data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2638fce",
   "metadata": {},
   "source": [
    "**(c)** Train a neural network with a single hidden layer on the dataset, tuning the relevant hyperparameters to optimize accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0cc1d53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6118 - loss: 0.6500 - val_accuracy: 0.7423 - val_loss: 0.5583\n",
      "Epoch 2/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7490 - loss: 0.5434 - val_accuracy: 0.7890 - val_loss: 0.4900\n",
      "Epoch 3/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7926 - loss: 0.4818 - val_accuracy: 0.8097 - val_loss: 0.4445\n",
      "Epoch 4/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8179 - loss: 0.4384 - val_accuracy: 0.8283 - val_loss: 0.4142\n",
      "Epoch 5/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8334 - loss: 0.4063 - val_accuracy: 0.8337 - val_loss: 0.3926\n",
      "Epoch 6/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8452 - loss: 0.3810 - val_accuracy: 0.8440 - val_loss: 0.3761\n",
      "Epoch 7/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8527 - loss: 0.3604 - val_accuracy: 0.8483 - val_loss: 0.3629\n",
      "Epoch 8/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8619 - loss: 0.3430 - val_accuracy: 0.8483 - val_loss: 0.3521\n",
      "Epoch 9/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8690 - loss: 0.3278 - val_accuracy: 0.8527 - val_loss: 0.3431\n",
      "Epoch 10/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8747 - loss: 0.3145 - val_accuracy: 0.8553 - val_loss: 0.3357\n",
      "Epoch 11/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8823 - loss: 0.3026 - val_accuracy: 0.8590 - val_loss: 0.3295\n",
      "Epoch 12/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8879 - loss: 0.2918 - val_accuracy: 0.8623 - val_loss: 0.3244\n",
      "Epoch 13/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8923 - loss: 0.2820 - val_accuracy: 0.8643 - val_loss: 0.3202\n",
      "Epoch 14/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8976 - loss: 0.2727 - val_accuracy: 0.8670 - val_loss: 0.3170\n",
      "Epoch 15/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9013 - loss: 0.2638 - val_accuracy: 0.8680 - val_loss: 0.3149\n",
      "Epoch 16/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9051 - loss: 0.2551 - val_accuracy: 0.8697 - val_loss: 0.3138\n",
      "Epoch 17/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9084 - loss: 0.2465 - val_accuracy: 0.8707 - val_loss: 0.3139\n",
      "Epoch 18/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9114 - loss: 0.2382 - val_accuracy: 0.8713 - val_loss: 0.3156\n",
      "Epoch 19/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9151 - loss: 0.2303 - val_accuracy: 0.8713 - val_loss: 0.3190\n",
      "Epoch 20/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9175 - loss: 0.2230 - val_accuracy: 0.8700 - val_loss: 0.3242\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "from numpy.random import seed, randint\n",
    "\n",
    "seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "model = Sequential() #initialize neural network\n",
    "model.add(Dense(units = 30, activation = 'sigmoid', input_dim = 10000)) #add the first hidden layer\n",
    "model.add(Dense(units = 2, activation = 'softmax')) #output layer\n",
    "\n",
    "sgd = optimizers.SGD(learning_rate = 0.03)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 20, batch_size = 50, validation_split = 0.2, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4106b76d",
   "metadata": {},
   "source": [
    "I messed around with the Learning Rate, Curve type, number of epochs and batch size until I achieved 91% accuracy on the test set and 87% accuracy on the validation set.\n",
    "\n",
    "What happened first was that I was looking at the train accuracy and realized that if I added enough epochs it would eventually reach 100%. So I capped the epochs at the point where it reached 100% training accuracy. However, I then started keeping my eye in the validation accuracy and realized that this one \"plateau'd\" far sooner, which made me reduce the epoch number even further. 87% for the validation accuracy was the absolyte maximum I managed to reach across all experiments.\n",
    "\n",
    "I then realized that if my learning rate was too high, there would be considerate fluctuations (both up and down) across my epochs. Considering that I had so little epochs, I decided to reduce the learning rate so that I would have a higher chance of increasing the accuracy with each epoch. This worked out well for the hyperparameters I ended up going with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd327a6",
   "metadata": {},
   "source": [
    "**(d)** Test your sentiment-classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7a70c996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - accuracy: 0.8599 - loss: 0.3350\n",
      "Test Loss: 0.3363731801509857\n",
      "Test Accuracy: 0.8611999750137329\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154becb6",
   "metadata": {},
   "source": [
    "It seems that the test accuracy is extremely similar to the validation accuracy, which is good!\n",
    "\n",
    "It means that the model is good at generalizing on unseen data and that the split between train/test/validation was good as well, since there was no considerable fluctutation between the test and validation accuracy results.\n",
    "\n",
    "Since I couldn't bring my validation accuracy any higher, I also didn't expect the test accuracy to go higher than what we got in the previous step.\n",
    "\n",
    "I am happy with these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44ee62",
   "metadata": {},
   "source": [
    "**(e)** Use the classifier to classify a few sentences you write yourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a1ef2970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "[0.86603284 0.13396718]\n",
      "Original Text: This movie was terrible! The acting was awful.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.28333795 0.71666205]\n",
      "Original Text: Amazing movie!\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.43251127 0.56748873]\n",
      "Original Text: Cate Blanchett\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.29950377 0.7004962 ]\n",
      "Original Text: I have seen more boobs in this movie than in my life\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.3316189 0.6683811]\n",
      "Original Text: I love the authenticity of the actors\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.3597052  0.64029485]\n",
      "Original Text: a lot of violence\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.5069899  0.49301013]\n",
      "Original Text: too much violence\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.46559173 0.5344083 ]\n",
      "Original Text: i hated this movie\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.64204276 0.35795733]\n",
      "Original Text: Adam Sandler is ugly and acts bad\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.39344728 0.6065528 ]\n",
      "Original Text: I actually cried\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.2951782 0.7048217]\n",
      "Original Text: beautiful animation\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.44732192 0.5526781 ]\n",
      "Original Text: This studio only does shitty crappy movies\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.618675   0.38132504]\n",
      "Original Text: terrible\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.60251623 0.39748377]\n",
      "Original Text: This studio only does terrible movies\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.41482466 0.5851752 ]\n",
      "Original Text: I hate this movie\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.46559173 0.5344083 ]\n",
      "Original Text: I hated this movie\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.5584128 0.4415872]\n",
      "Original Text: I hated hated hated hated this movie\n",
      "Predicted Sentiment: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_text = [\n",
    "    \"This movie was terrible! The acting was awful.\",\n",
    "    \"Amazing movie!\",\n",
    "    \"Cate Blanchett\",\n",
    "    \"I have seen more boobs in this movie than in my life\",\n",
    "    \"I love the authenticity of the actors\",\n",
    "    \"a lot of violence\",\n",
    "    \"too much violence\",\n",
    "    \"i hated this movie\",\n",
    "    \"Adam Sandler is ugly and acts bad\",\n",
    "    \"I actually cried\",\n",
    "    \"beautiful animation\",\n",
    "    \"This studio only does shitty crappy movies\",\n",
    "    \"terrible\",\n",
    "    \"This studio only does terrible movies\",\n",
    "    \"I hate this movie\",\n",
    "    \"I hated this movie\",\n",
    "    \"I hated hated hated hated this movie\"]\n",
    "\n",
    "\n",
    "def print_models_preds(newtextarray):\n",
    "    new_text_vectorized = vectorizer.transform(newtextarray)\n",
    "\n",
    "    predictions = model.predict(new_text_vectorized)\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        print(pred)\n",
    "        sentiment = \"positive\" if pred[0] < 0.5 else \"negative\"\n",
    "        print(f\"Original Text: {newtextarray[i]}\")\n",
    "        print(f\"Predicted Sentiment: {sentiment}\\n\")\n",
    "    \n",
    "print_models_preds(new_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c22d0a",
   "metadata": {},
   "source": [
    "By printing out the predicted sentiment alongside the model's confidence for each of these reviews, we can conclude that the \"Bag of Words\" method might not be the best way to go for this exercise because perhaps it removes emphasis from the words that actually matter for the sentiment of the review. An average person would immediatelly determine \"I hated this movie\" and \"This studio only does shitty crappy movies\" as negative reviews, but the model, despite being a bit \"on edge\" (only around 60% sure), still classified them as positive.\n",
    "\n",
    "From testing with my own sentences, I also realized that the amount of times that a word appears in a review greatly influences the output of this model (\"I hated this movie\" being classified as positive but \"I hated hated hated hated this movie\" being classified as negative).\n",
    "\n",
    "One way to make this model more robust would be to remove neutral words from the bag of words (for example \"I\", \"movie\", \"this\", \"is\") and all of these words that are required to make a sentence make sense. Then, the remaining words would carry far more meaning to the model, perhaps making it perform a lot better (Because the sentence \"I hated this movie\" would have just become \"hated\" and be far more likely to be classified as negative).\n",
    "\n",
    "I would also like to note that perhaps the reason why the model didn't perform so well on my hand-written reviews is because they are drastically shorter than those we see on imdb, so this type of data might be a bit foreign to our model ahahahah.\n",
    "\n",
    "Having observed this, I want to try it with actual imdb reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bdc62e",
   "metadata": {},
   "source": [
    "Below, I have added three positive and three negative reviews (10 stars and 1 star) from Everything Everywhere All at Once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c8e2df8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "[0.1804981 0.8195018]\n",
      "Original Text: I have trouble turning off my brain. Anxieties, worries, mundane to-dos, even positive things, sometimes feel like they're swirling around in a chaotic funnel cloud and I would like nothing more than to sit in physical and mental silence. Everything Everywhere All At Once felt like the inside of my head. In a world of non-stop, 24/7 news, most of it bad, how is a person like me, who has trouble filtering out things that affect me directly from all of the other things that are just out there happening in general and over which I have no control, supposed to cope? One answer is to decide that nothing matters anyway and give up caring. But that means deciding that my wife doesn't matter. And that my kids don't matter. And that art, and nature, and things that bring joy to my life, don't matter. Another way is to decide that some things, ok maybe most things, don't matter, but that there are things that do, and those are the things that make it all worth it. I get to decide what those things are. The first approach is nihilistic. The second approach is empowering. This film explores both approaches, and I was a sobbing mess at the end. I will say there were times that I was a bit exhausted by this movie. It throws a lot on the screen and at the viewer, and occasionally it can't keep up with its ambitions. But this was mostly a home run. Michell Yeoh does terrific work in this, but the MVP is Ke Huy Quan (Short Round from the Indiana Jones movies).\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.00216325 0.9978367 ]\n",
      "Original Text: Be kind, especially when you don't know what's going on. If only we could recognize that those who combat us in life do so out of hidden pain, and fight them with kindness. If only we could accept our kids for who they are, and say supportive things from the heart instead of trying to mold them. If only we could be content with the life we have, and set aside the idea of the countless other lives we might have led, had we made different decisions along the way. If only we could see that the flipside to life being meaningless and everything ultimately being sucked into the abyss is the freedom that comes from that, that we can do anything with the time we've got. Like the first word in its title, this film feels like everything. While watching it I thought Stephanie Hsu was everything too, but then again, so was Michelle Yeoh and Jamie Lee Curtis. The homage to Wong Kar-Wai with Ke Huy Quan making his enlightened speech is extraordinary, and the fact that James Hong was still getting it done at age 93 was wonderful. I also loved how both mother and daughter, troubled as they were with their upbringing, each found comfort in a kind and patient partner, and Tallie Medel's character was a nice touch. The film threatens to go off the rails with its madcap multiverse hopping in the first part, but it's all a setup for the second part, which is incredibly powerful. It was impressive that it managed to be so entertaining along the way, with its fast-paced action and the wild ride it takes us on. It's one that rewards a rewatch to, as its full of little details and references. Just a great film, lots of fun, and from the heart.\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.00224914 0.9977509 ]\n",
      "Original Text: Everything Everywhere All At Once is even crazier than the trailer would lead one to believe. It's bursting with so many original, weird concepts and ideas that no serious storyteller in their right mind would ever dare to put in their movie. But Daniels did. And it works with spectacularly effective results. These filmmakers choose not only to put every bizarre idea they could think of into their movie, but they ensure that every oddity adds something to the thought-provoking, emotionally resonant themes that pervade the story. So many angles can be explored surrounding this story of emotional connection and the things that distract from it. As Evelyn connects with her alternate selves and alternate relatives, there is both envy of and pity toward them that is explored. It's such a cathartic experience to watch this woman discover what she truly wants from life and loves about her life. All of the beautiful themes and heartwarming character moments are just the tip of the iceberg here, though. This is one of the funniest movies I have ever seen. The Daniels lean into the absurdity of the world that they have created, always reaching for the most outlandish possibilities and never playing it safe. It makes for some truly outstanding visual gags that will never leave my mind., and it makes for the most unique viewing experience I have ever had in my life. This is a masterpiece of originality. There is not, nor will there ever be, a film quite like this one. And I cannot wait to watch it again and again and again.\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.40034014 0.59965986]\n",
      "Original Text: This movie was supposed to be the best ever, but it just freaked me out. The movie is all over the place and the depth it could have had is ruined by randomness, absurd humor and pretenciousness. In one of the scenes the characters suddenly have sausages instead of fingers and they make a pretensious referance to A space odyssey where the pre human apes fight with sausage fingers. Maybe that's just humourous and well meant, but I could hardly watch. It's impossible to tell if this movie is a comedy or an action movie or a deep drama and if such confusion is your thing, then maybe you would love it. I could barely stand it.\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "[0.7363564 0.2636436]\n",
      "Original Text: I cannot understand why this movie got high reviews. It is overly strange but not in a good way. It's strange in a way that makes absolutely no sense. There's no good acting or effects. The story is almost non-existing. There is no cohesion and no characters that makes sense at all. It may have some famous characters or a famous director, I'm not sure. I recognized the main character only. It's like someone did a school project while doing drugs and ended with this weird but bad result. It's too far fetched even from an artistic point of view. I cannot recommend it no matter what preferences you may have.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.8793353 0.1206647]\n",
      "Original Text: I watch and re-watch a lot of movies per year as a hobby (no money involved), and today I have 9988 reviews in IMDb. Last month, I received an email from IMDb listing Everything Everywhere All at Once as one of the Top-10 movies of 2022. In IMDb, it is informed that this flick is nominated for 10 BAFTA Awards, 240 wins and 351 nominations. I can only understand that this is a heard behavior to the promotion of studio, using professional critics and press to promote such a garbage. I cannot envision a normal being, without financial interest or being manipulated by critics, to enjoy this crap. It seems to be a bad trip of the writers turned into a movie by insane productors. In the end, this film is 2h 19 min of complete waste of time. My vote is one (awful).\n",
      "Predicted Sentiment: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_text = [\n",
    "    \"I have trouble turning off my brain. Anxieties, worries, mundane to-dos, even positive things, sometimes feel like they're swirling around in a chaotic funnel cloud and I would like nothing more than to sit in physical and mental silence. Everything Everywhere All At Once felt like the inside of my head. In a world of non-stop, 24/7 news, most of it bad, how is a person like me, who has trouble filtering out things that affect me directly from all of the other things that are just out there happening in general and over which I have no control, supposed to cope? One answer is to decide that nothing matters anyway and give up caring. But that means deciding that my wife doesn't matter. And that my kids don't matter. And that art, and nature, and things that bring joy to my life, don't matter. Another way is to decide that some things, ok maybe most things, don't matter, but that there are things that do, and those are the things that make it all worth it. I get to decide what those things are. The first approach is nihilistic. The second approach is empowering. This film explores both approaches, and I was a sobbing mess at the end. I will say there were times that I was a bit exhausted by this movie. It throws a lot on the screen and at the viewer, and occasionally it can't keep up with its ambitions. But this was mostly a home run. Michell Yeoh does terrific work in this, but the MVP is Ke Huy Quan (Short Round from the Indiana Jones movies).\",\n",
    "    \"Be kind, especially when you don't know what's going on. If only we could recognize that those who combat us in life do so out of hidden pain, and fight them with kindness. If only we could accept our kids for who they are, and say supportive things from the heart instead of trying to mold them. If only we could be content with the life we have, and set aside the idea of the countless other lives we might have led, had we made different decisions along the way. If only we could see that the flipside to life being meaningless and everything ultimately being sucked into the abyss is the freedom that comes from that, that we can do anything with the time we've got. Like the first word in its title, this film feels like everything. While watching it I thought Stephanie Hsu was everything too, but then again, so was Michelle Yeoh and Jamie Lee Curtis. The homage to Wong Kar-Wai with Ke Huy Quan making his enlightened speech is extraordinary, and the fact that James Hong was still getting it done at age 93 was wonderful. I also loved how both mother and daughter, troubled as they were with their upbringing, each found comfort in a kind and patient partner, and Tallie Medel's character was a nice touch. The film threatens to go off the rails with its madcap multiverse hopping in the first part, but it's all a setup for the second part, which is incredibly powerful. It was impressive that it managed to be so entertaining along the way, with its fast-paced action and the wild ride it takes us on. It's one that rewards a rewatch to, as its full of little details and references. Just a great film, lots of fun, and from the heart.\",\n",
    "    \"Everything Everywhere All At Once is even crazier than the trailer would lead one to believe. It's bursting with so many original, weird concepts and ideas that no serious storyteller in their right mind would ever dare to put in their movie. But Daniels did. And it works with spectacularly effective results. These filmmakers choose not only to put every bizarre idea they could think of into their movie, but they ensure that every oddity adds something to the thought-provoking, emotionally resonant themes that pervade the story. So many angles can be explored surrounding this story of emotional connection and the things that distract from it. As Evelyn connects with her alternate selves and alternate relatives, there is both envy of and pity toward them that is explored. It's such a cathartic experience to watch this woman discover what she truly wants from life and loves about her life. All of the beautiful themes and heartwarming character moments are just the tip of the iceberg here, though. This is one of the funniest movies I have ever seen. The Daniels lean into the absurdity of the world that they have created, always reaching for the most outlandish possibilities and never playing it safe. It makes for some truly outstanding visual gags that will never leave my mind., and it makes for the most unique viewing experience I have ever had in my life. This is a masterpiece of originality. There is not, nor will there ever be, a film quite like this one. And I cannot wait to watch it again and again and again.\",\n",
    "    \"This movie was supposed to be the best ever, but it just freaked me out. The movie is all over the place and the depth it could have had is ruined by randomness, absurd humor and pretenciousness. In one of the scenes the characters suddenly have sausages instead of fingers and they make a pretensious referance to A space odyssey where the pre human apes fight with sausage fingers. Maybe that's just humourous and well meant, but I could hardly watch. It's impossible to tell if this movie is a comedy or an action movie or a deep drama and if such confusion is your thing, then maybe you would love it. I could barely stand it.\",\n",
    "    \"I cannot understand why this movie got high reviews. It is overly strange but not in a good way. It's strange in a way that makes absolutely no sense. There's no good acting or effects. The story is almost non-existing. There is no cohesion and no characters that makes sense at all. It may have some famous characters or a famous director, I'm not sure. I recognized the main character only. It's like someone did a school project while doing drugs and ended with this weird but bad result. It's too far fetched even from an artistic point of view. I cannot recommend it no matter what preferences you may have.\",\n",
    "    \"I watch and re-watch a lot of movies per year as a hobby (no money involved), and today I have 9988 reviews in IMDb. Last month, I received an email from IMDb listing Everything Everywhere All at Once as one of the Top-10 movies of 2022. In IMDb, it is informed that this flick is nominated for 10 BAFTA Awards, 240 wins and 351 nominations. I can only understand that this is a heard behavior to the promotion of studio, using professional critics and press to promote such a garbage. I cannot envision a normal being, without financial interest or being manipulated by critics, to enjoy this crap. It seems to be a bad trip of the writers turned into a movie by insane productors. In the end, this film is 2h 19 min of complete waste of time. My vote is one (awful).\",\n",
    "]\n",
    "\n",
    "print_models_preds(imdb_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f6550",
   "metadata": {},
   "source": [
    "The model does a great job at classifying positive reviews with EXTREME confidence, but is far less confident when it comes to negative reviews, even misclassifying the first negative one.\n",
    "\n",
    "I also noticed, with my naked eye, that the positive reviews are far longer than the negative. Could it be that, due to using the Bag of Words method, the method always leans more towards positive reviews since those contain more words and so the sentiment of positivity is associated with far more words than the negative sentiment?\n",
    "\n",
    "Using real reviews made the analysis of this model even more interesting!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135e5e0",
   "metadata": {},
   "source": [
    "I couldn't resist but to also add reviews of 5 stars for the same movie... I thought their sentiment would be so-so, but they are actually quite negative! Some of them felt even more negative than the one-star ones. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6e00ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[0.53321844 0.46678147]\n",
      "Original Text: This one is a meh film, someone's alternate idea of multi-verse sci-fi and how do we script things to get Michelle Yeoh doing martial arts again ? A long, long way to push a theme about LGBT acceptance. The anal plugs and dildos were just odd and a strange inclusion. Some of the characters were outright annoying such as the daughter and the various minions. The hubby was OK as he smoothly switched personalities. Good old James Hong still with us, always recall him as hey that guy.... Jamie Lee Curtis is fine as the IRS Auditor hamming it up. And Yeoh going through the paces, looks like a bit of a payday for her. All in all certainly not one of the better films of the year.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.97885555 0.02114445]\n",
      "Original Text: The onslaught of Visual Metaphors gets old, and one is looking for the film to just get it over with and end. So disappointing - doesn't live up to expectations nor the hype. First half has some engaging moments, but it could have wrapped up scenes without explaining what we already knew. I don't need a primer to explain the content and themes of a film thank you very much.This film could have been so much better, but it fails to rely upon the intricate visuals (so painstakingly laid out), deciding instead we needed explanation of the filmmakers' intent. Such a hedge essentially ruined this film - it is awkward and runs about 45 minutes too long because of it. Nice effort visually - my rating is frankly higher than I feel it should be, but the visuals were often witty, so...5.Overall, I had to force myself to see entire film, and it was painful. I feel cheated out of what could have been. Perhaps one Director would have made a difference. Don't know. Won't ever know. It's a self-indulgent mess.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.81421125 0.1857888 ]\n",
      "Original Text: Another example of a incoherent and weak storytelling, weird shifts in tempo and bad exposition after people become even more confused.Besides two fight scenes I was bored and annoyed to death by the film. There was only one scene I enjoyed very much about two stones sitting on a ledge of a canyon.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "[0.690696   0.30930403]\n",
      "Original Text: I had high hopes for this film. I love Michelle Y. She is a brilliant and talented actor. However this film is a big let down. Her acting is excellent as usual. The plot is all over the place. It is random to the point of feeling like some sort of bizarre circus show from an alternative Universe.I am completely baffled by the rating and can only concur that it's high because of the pretty high standard of visual effects and constant barrage of random action.I Felt like I was eating 10 portions of sticky toffee pudding and needed to vomit by the end but I was to delirious to find a place to release the contents of my stomach and therefore had to hold it in. Not a great feeling!This film had huge potential. The cast is great. It ended up feeling lazy and confusing. Maybe that was the intended effect? Maybe this is a film that is feeding into the random Instagram/Tik Tok scrolling generation?For those with huge attention deficit this film might be a blessing.Not for me I'm afraid. A big waste of talent.You know that feeling when someone starts telling you an anecdote and you think its going to be funny, but they just go on and on until eventually you switch off to, what they're saying... that's how this film was for me. I was laughing out loud for the first 20 minutes, enjoying the bizarreness of it all, then I was smiling, then just watching... after an hour I'd picked up my iPad and was checking emails, glancing up at the TV screen, where the nonsense was still trying to make me laugh. But it was just more nonsense. When The End came up, it was a relief. I know there was 50 mins+ left, but I really didn't care. Gave up and watched something else.\n",
      "Predicted Sentiment: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "five_star_text = [\n",
    "    \"This one is a meh film, someone's alternate idea of multi-verse sci-fi and how do we script things to get Michelle Yeoh doing martial arts again ? A long, long way to push a theme about LGBT acceptance. The anal plugs and dildos were just odd and a strange inclusion. Some of the characters were outright annoying such as the daughter and the various minions. The hubby was OK as he smoothly switched personalities. Good old James Hong still with us, always recall him as hey that guy.... Jamie Lee Curtis is fine as the IRS Auditor hamming it up. And Yeoh going through the paces, looks like a bit of a payday for her. All in all certainly not one of the better films of the year.\",\n",
    "    \"The onslaught of Visual Metaphors gets old, and one is looking for the film to just get it over with and end. So disappointing - doesn't live up to expectations nor the hype. First half has some engaging moments, but it could have wrapped up scenes without explaining what we already knew. I don't need a primer to explain the content and themes of a film thank you very much.This film could have been so much better, but it fails to rely upon the intricate visuals (so painstakingly laid out), deciding instead we needed explanation of the filmmakers' intent. Such a hedge essentially ruined this film - it is awkward and runs about 45 minutes too long because of it. Nice effort visually - my rating is frankly higher than I feel it should be, but the visuals were often witty, so...5.Overall, I had to force myself to see entire film, and it was painful. I feel cheated out of what could have been. Perhaps one Director would have made a difference. Don't know. Won't ever know. It's a self-indulgent mess.\",\n",
    "    \"Another example of a incoherent and weak storytelling, weird shifts in tempo and bad exposition after people become even more confused.Besides two fight scenes I was bored and annoyed to death by the film. There was only one scene I enjoyed very much about two stones sitting on a ledge of a canyon.\",\n",
    "    \"I had high hopes for this film. I love Michelle Y. She is a brilliant and talented actor. However this film is a big let down. Her acting is excellent as usual. The plot is all over the place. It is random to the point of feeling like some sort of bizarre circus show from an alternative Universe.I am completely baffled by the rating and can only concur that it's high because of the pretty high standard of visual effects and constant barrage of random action.I Felt like I was eating 10 portions of sticky toffee pudding and needed to vomit by the end but I was to delirious to find a place to release the contents of my stomach and therefore had to hold it in. Not a great feeling!This film had huge potential. The cast is great. It ended up feeling lazy and confusing. Maybe that was the intended effect? Maybe this is a film that is feeding into the random Instagram/Tik Tok scrolling generation?For those with huge attention deficit this film might be a blessing.Not for me I'm afraid. A big waste of talent.\"\n",
    "    \"You know that feeling when someone starts telling you an anecdote and you think its going to be funny, but they just go on and on until eventually you switch off to, what they're saying... that's how this film was for me. I was laughing out loud for the first 20 minutes, enjoying the bizarreness of it all, then I was smiling, then just watching... after an hour I'd picked up my iPad and was checking emails, glancing up at the TV screen, where the nonsense was still trying to make me laugh. But it was just more nonsense. When The End came up, it was a relief. I know there was 50 mins+ left, but I really didn't care. Gave up and watched something else.\"\n",
    "]\n",
    "\n",
    "print_models_preds(five_star_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53120c91",
   "metadata": {},
   "source": [
    "Our model caught all of them! So it does perform pretty good on real reviews from imdb. Maybe we should feet it YouTube comments too next time and it'll do better with the short-form commentary ahahahah.\n",
    "\n",
    "This assignment was super interesitng :)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
